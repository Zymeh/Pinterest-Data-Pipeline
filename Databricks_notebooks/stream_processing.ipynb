{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructField, StringType, StructType, TimestampType, IntegerType\n",
    "\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Kinesis Streams\n",
    "\n",
    "First, we are reading the credential file, and extracting the keys we need. \n",
    "\n",
    "> Note that we are also encoding the SECERET_KEY for privacy reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying CSV file format with headers and comma as the delimiter.\n",
    "file_type = \"csv\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a function that reads the kinesis data, and transforms it into a usable dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kinesis_data(stream_name, json_schema):\n",
    "    ''' A function that reads the Kinesis data and outputs a usable dataframe.\n",
    "\n",
    "    This function reads the kinesis stream using `spark`. It then deserialises it by:\n",
    "    extracting the 'data' field from the Kinesis stream and casts it to a string, parses \n",
    "    the 'data' field using the specified json shcema, then selects all columns from the \n",
    "    parsed data.\n",
    "\n",
    "    Args:\n",
    "        stream_name (`string`): The name of the stream we wish to stream data from.\n",
    "        json_schema (`StructType`): The desired schema of the output dataframe.\n",
    "\n",
    "    Returns:\n",
    "        df (`pyspark.sql.DataFrame`): A DataFrame with the specified schema and alias.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Creating the kinesis_stream dataframe\n",
    "    kinesis_stream = spark \\\n",
    "        .readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName', stream_name) \\\n",
    "        .option('initialPosition', 'earliest') \\\n",
    "        .option('region', 'us-east-1') \\\n",
    "        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "        .option('awsSecretKey', SECRET_KEY) \\\n",
    "        .load()\n",
    "\n",
    "    # deserialising the data\n",
    "    df = kinesis_stream \\\n",
    "    .selectExpr(\"CAST(data as STRING)\") \\ \n",
    "    .withColumn(\"data\", from_json(col(\"data\"), json_schema)) \\ \n",
    "    .select(col(\"data.*\")) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to Delta Tables\n",
    "\n",
    "We will now create a function which writes the transformed data to appropriate delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_kinesis_data(table_name, df):\n",
    "    ''' A function that writes data to the specified delta table, it also \n",
    "    checkpoints the data incase we need to roll it back to an older state.\n",
    "\n",
    "    Args:\n",
    "        table_name (`string`): The desired name for the delta table\n",
    "        df (`pyspark.sql.DataFrame`): \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df.writeStream \\ \n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\ \n",
    "    .option(\"checkpointLocation\", f\"/tmp/kinesis/{table_name}_checkpoints/\") \\ \n",
    "    .table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "Now we read the pin streaming data, cleaning it then uploading it to the appropriate delta table. I have removed sensitive data too, for example we should have:\n",
    "\n",
    "```\n",
    "stream_name = '<name_of_kinesis_stream>'\n",
    "```\n",
    "and also:\n",
    "\n",
    "```\n",
    "table_name = '<desired_delta_table_name>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the stream_name\n",
    "stream_name = '<name_of_kinesis_stream>'\n",
    "\n",
    "# defining the schema for the json data\n",
    "json_schema = StructType([\n",
    "    StructField('index', IntegerType()),\n",
    "    StructField('unique_id', StringType()),\n",
    "    StructField('title', StringType()),\n",
    "    StructField('description', StringType()),\n",
    "    StructField('follower_count', IntegerType()),\n",
    "    StructField('poster_name', StringType()),\n",
    "    StructField('tag_list', StringType()),\n",
    "    StructField('is_image_or_video', StringType()),\n",
    "    StructField('image_src', StringType()),\n",
    "    StructField('save_location', StringType()),\n",
    "    StructField('category', StringType())\n",
    "])\n",
    "\n",
    "df_pin = read_kinesis_data(stream_name, json_schema)\n",
    "\n",
    "# replacing invalid entries with `None`, it is best to define a dictionary \n",
    "# here as this makes the whole process more scalable. \n",
    "\n",
    "col_and_entries_to_replace = {\n",
    "    'description' : 'No description available Story format',\n",
    "    'follower_count' : 'User Info Error',\n",
    "    'image_src' : 'Image src error.',\n",
    "    'poster_name' : 'User Info Error',\n",
    "    'tag_list' : 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e',\n",
    "    'title' : 'No Title Data Available'\n",
    "}\n",
    "\n",
    "for column, value in col_and_entries_to_replace.items():\n",
    "    df_pin = df_pin.withColumn(column, when(df_pin[column] == value, None).otherwise(df_pin[column]))\n",
    "\n",
    "# replacing k and M with 000 and 000000 respectivly.\n",
    "df_pin = (df_pin\n",
    "    .withColumn('follower_count', \n",
    "        when(df_pin.follower_count.endswith('k'), regexp_replace(df_pin.follower_count, 'k', '000'))\n",
    "        .when(df_pin.follower_count.endswith('M'), regexp_replace(df_pin.follower_count, 'M', '000000'))\n",
    "        .otherwise(df_pin.follower_count))\n",
    ")\n",
    "\n",
    "# casting folower count to integers\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin.follower_count.cast('int'))\n",
    "\n",
    "# making the 'save_location' column show the path\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace( 'save_location', 'Local save in ', ''))\n",
    "\n",
    "# defining the delta table name.\n",
    "table_name = '<desired_delta_table_name>'\n",
    "\n",
    "# writing the data to the delta tables. \n",
    "write_kinesis_data(table_name, df_pin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the geo streaming data, cleaning it then uploading it to the appropriate delta table. I have removed sensitive data too, for example we should have:\n",
    "\n",
    "```\n",
    "stream_name = '<name_of_kinesis_stream>'\n",
    "```\n",
    "and also:\n",
    "\n",
    "```\n",
    "table_name = '<desired_delta_table_name>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the name of the kinesis stream we want to clean\n",
    "stream_name = '<name_of_kinesis_stream>'\n",
    "\n",
    "# defining the schema for the json data\n",
    "json_schema = StructType([\n",
    "    StructField('ind', IntegerType()),\n",
    "    StructField('country', StringType()),\n",
    "    StructField('latitude', StringType()),\n",
    "    StructField('longitude', StringType()),\n",
    "    StructField('timestamp', TimestampType())\n",
    "])\n",
    "\n",
    "# using the function defined above.\n",
    "df_geo = read_kinesis_data(stream_name, json_schema)\n",
    "\n",
    "# creating a new column called 'coordinates'\n",
    "df_geo = df_geo.withColumn('coordinates', array(df_geo.latitude, df_geo.longitude))\n",
    "\n",
    "# dropping the columns 'latitude' and 'longitude'\n",
    "df_geo = df_geo.drop(*['latitude', 'longitude'])\n",
    "\n",
    "# converting 'timestamp' to a timestamp data type.\n",
    "df_geo = df_geo.withColumn('timestamp', to_timestamp(df_geo.timestamp))\n",
    "\n",
    "# defining the delta table name.\n",
    "table_name = '<desired_delta_table_name>'\n",
    "\n",
    "# writing the data to the delta tables. \n",
    "write_kinesis_data(table_name, df_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the user streaming data, cleaning it then uploading it to the appropriate delta table. I have removed sensitive data too, for example we should have:\n",
    "\n",
    "```\n",
    "stream_name = '<name_of_kinesis_stream>'\n",
    "```\n",
    "and also:\n",
    "\n",
    "```\n",
    "table_name = '<desired_delta_table_name>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the name of the kinesis stream we want to clean\n",
    "stream_name = '<name_of_kinesis_stream>'\n",
    "\n",
    "# defining the schema for the json data\n",
    "json_schema = StructType([\n",
    "    StructField('ind', IntegerType()),\n",
    "    StructField('first_name', StringType()),\n",
    "    StructField('last_name', StringType()),\n",
    "    StructField('age', IntegerType()),\n",
    "    StructField('date_joined', TimestampType())\n",
    "])\n",
    "\n",
    "\n",
    "# using the function defined above.\n",
    "df_user = read_kinesis_data(stream_name, json_schema)\n",
    "\n",
    "# creating a new column called 'user_name'\n",
    "df_user = df_user.withColumn('user_name', concat(df_user.first_name, lit(' '), df_user.last_name))\n",
    "\n",
    "# dropping the 'first_name' and 'last_name' columns\n",
    "df_user = df_user.drop(*['first_name', 'last_name'])\n",
    "\n",
    "# converting 'date_joined' to a timestamp data type.\n",
    "df_user = df_user.withColumn('date_joined', to_timestamp(df_user.date_joined))\n",
    "\n",
    "# reordering columns\n",
    "df_user = df_user.select('ind', 'user_name', 'age', 'date_joined')\n",
    "\n",
    "# defining the delta table name.\n",
    "table_name = '<desired_delta_table_name>'\n",
    "\n",
    "# writing the data to the delta tables. \n",
    "write_kinesis_data(table_name, df_user)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
