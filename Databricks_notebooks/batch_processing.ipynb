{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructField, StringType, StructType, TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I created a function which reads the data from the S3 bucket and loads it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_mount_data(mount_name):\n",
    "    ''' A function which reads data from the desired S3 bucket, and returns a \n",
    "    usable DataFrame.\n",
    "\n",
    "    Args:\n",
    "        mount_name ('string'): Corresponds to the name you gave to the mounted \n",
    "        S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        df (`pyspark.sql.DataFrame`): A DataFrame which is ready to be cleaned \n",
    "        and queried.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    file_location = f\"/mnt/{mount_name}/*.json\"\n",
    "    file_type = \"json\"\n",
    "    infer_schema = \"true\"\n",
    "    df = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(file_location)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are reading the pin data and cleaning it, I have removed sensitive data too, for example we should have:\n",
    "\n",
    "```\n",
    "df_pin = read_s3_mount_data(<mount_name>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df_pin = read_s3_mount_data(<mount_name>)\n",
    "\n",
    "# replacing invalid entries with `None`, it is best to define a dictionary \n",
    "# here as this makes the whole process more scalable. \n",
    "\n",
    "col_and_entries_to_replace = {\n",
    "    'description' : 'No description available Story format',\n",
    "    'follower_count' : 'User Info Error',\n",
    "    'image_src' : 'Image src error.',\n",
    "    'poster_name' : 'User Info Error',\n",
    "    'tag_list' : 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e',\n",
    "    'title' : 'No Title Data Available'\n",
    "}\n",
    "\n",
    "for column, value in col_and_entries_to_replace.items():\n",
    "    df_pin = df_pin.withColumn(column, when(df_pin[column] == value, None).otherwise(df_pin[column]))\n",
    "\n",
    "# replacing k and M with 000 and 000000 respectivly.\n",
    "df_pin = (df_pin\n",
    "    .withColumn('follower_count', \n",
    "        when(df_pin.follower_count.endswith('k'), regexp_replace(df_pin.follower_count, 'k', '000'))\n",
    "        .when(df_pin.follower_count.endswith('M'), regexp_replace(df_pin.follower_count, 'M', '000000'))\n",
    "        .otherwise(df_pin.follower_count))\n",
    ")\n",
    "\n",
    "# casting folower count to integers\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin.follower_count.cast('int'))\n",
    "\n",
    "# renaming index to ind so it is consistent with the other tables.\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "\n",
    "# making the 'save_location' column show the path\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace( 'save_location', 'Local save in ', ''))\n",
    "\n",
    "# reordering columns\n",
    "df_pin = df_pin.select('ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are reading the geo data and cleaning it, I have removed sensitive data too, for example we should have:\n",
    "\n",
    "```\n",
    "df_geo = read_s3_mount_data(<mount_name>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df_geo = read_s3_mount_data(<mount_name>)\n",
    "\n",
    "# creating a new column called 'coordinates'\n",
    "df_geo = df_geo.withColumn('coordinates', array(df_geo.latitude, df_geo.longitude))\n",
    "\n",
    "# dropping the columns 'latitude' and 'longitude'\n",
    "df_geo = df_geo.drop(*['latitude', 'longitude'])\n",
    "\n",
    "# converting 'timestamp' to a timestamp data type, and formatting it.\n",
    "df_geo = df_geo.withColumn('timestamp', to_timestamp(df_geo.timestamp))\n",
    "df_geo = df_geo.withColumn('timestamp', date_format('timestamp', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# reordering columns\n",
    "df_geo = df_geo.select('ind','country','coordinates','timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are reading the user data and cleaning it, I have removed sensitive data too, for example we should have:\n",
    "\n",
    "```\n",
    "df_user = read_s3_mount_data(<mount_name>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df_user = read_s3_mount_data(<mount_name>)\n",
    "\n",
    "# creating a new column called 'user_name'\n",
    "df_user = df_user.withColumn('user_name', concat(df_user.first_name, lit(' '), df_user.last_name))\n",
    "\n",
    "# dropping the 'first_name' and 'last_name' columns\n",
    "df_user = df_user.drop(*['first_name', 'last_name'])\n",
    "\n",
    "# converting 'date_joined' to a timestamp data type, and formatting it.\n",
    "df_user = df_user.withColumn('date_joined', to_timestamp(df_user.date_joined))\n",
    "df_user = df_user.withColumn('date_joined', date_format('date_joined', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# reordering columns\n",
    "df_user = df_user.select('ind', 'user_name', 'age', 'date_joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Data\n",
    "<small>The fun part!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most popular Pinterest category people post to based on their country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_pin.createOrReplaceTempView('df_temp_pin')\n",
    "df_geo.createOrReplaceTempView('df_temp_geo')\n",
    "\n",
    "# the SQL query\n",
    "\n",
    "#creating a cte\n",
    "cte_query = \"\"\"\n",
    "       WITH ranking_table AS (\n",
    "         SELECT df_temp_geo.country AS country,\n",
    "                df_temp_pin.category AS category,\n",
    "                COUNT(df_temp_pin.category) AS category_count,\n",
    "                RANK() OVER(PARTITION BY df_temp_geo.country ORDER BY COUNT(df_temp_pin.category) DESC) AS rank\n",
    "                FROM df_temp_geo\n",
    "                JOIN df_temp_pin ON df_temp_geo.ind = df_temp_pin.ind\n",
    "                GROUP BY df_temp_geo.country, df_temp_pin.category\n",
    "       )\n",
    "\"\"\"\n",
    "# using the above cte to create the desired output\n",
    "result_df_1 = spark.sql(cte_query + \"\"\"\n",
    "                    SELECT country,\n",
    "                           category,\n",
    "                           category_count\n",
    "                    FROM ranking_table\n",
    "                    WHERE rank == 1\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many posts each category had between 2018 and 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_pin.createOrReplaceTempView('df_temp_pin')\n",
    "df_geo.createOrReplaceTempView('df_temp_geo')\n",
    "\n",
    "# the SQL query\n",
    "\n",
    "#creating a cte\n",
    "cte_query = \"\"\"\n",
    "       WITH ranking_table AS (\n",
    "              SELECT year(df_temp_geo.timestamp) AS post_year,\n",
    "                     df_temp_pin.category AS category,\n",
    "                     COUNT(df_temp_pin.category) AS category_count,\n",
    "                     RANK() OVER (PARTITION BY year(df_temp_geo.timestamp) ORDER BY COUNT(df_temp_pin.category) DESC) AS rank\n",
    "              FROM df_temp_geo\n",
    "              JOIN df_temp_pin ON df_temp_geo.ind = df_temp_pin.ind  \n",
    "              WHERE 2018 <= year(df_temp_geo.timestamp) AND year(df_temp_geo.timestamp) <= 2022\n",
    "              GROUP BY year(df_temp_geo.timestamp), df_temp_pin.category     \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "result_df_2 = spark.sql(cte_query + \"\"\"\n",
    "                     SELECT post_year,\n",
    "                           category,\n",
    "                           category_count\n",
    "                     FROM ranking_table\n",
    "                     WHERE rank == 1\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the user with most followers with their corresponding country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_pin.createOrReplaceTempView('df_temp_pin')\n",
    "df_geo.createOrReplaceTempView('df_temp_geo')\n",
    "\n",
    "# SQL query \n",
    "cte_query = \"\"\"\n",
    "       WITH ranking_table AS (\n",
    "              SELECT df_temp_geo.country AS country,\n",
    "                     df_temp_pin.poster_name AS poster_name,\n",
    "                     df_temp_pin.follower_count AS follower_count,\n",
    "                     RANK() OVER(PARTITION BY df_temp_geo.country ORDER BY df_temp_pin.follower_count DESC) AS rank\n",
    "              FROM df_temp_pin\n",
    "              JOIN df_temp_geo ON df_temp_geo.ind = df_temp_pin.ind\n",
    "       )\n",
    "\"\"\"\n",
    "result_df_3 = spark.sql(cte_query + \"\"\"\n",
    "                    SELECT country,\n",
    "                           poster_name,\n",
    "                           follower_count\n",
    "                    FROM ranking_table\n",
    "                    WHERE rank == 1\n",
    "                    ORDER BY follower_count DESC\n",
    "                      \"\"\")\n",
    "                  \n",
    "result_df_3 = spark.createDataFrame([result_df_3.head(1)[0]])\n",
    "result_df_3 = result_df_3.drop('poster_name')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most popular category people post to based on age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_pin.createOrReplaceTempView('df_temp_pin')\n",
    "df_user.createOrReplaceTempView('df_temp_user')\n",
    "\n",
    "# SQL query \n",
    "\n",
    "# creating a cte to clear things up\n",
    "cte_query = \"\"\"\n",
    "    WITH ranking_table AS (\n",
    "        SELECT \n",
    "            category,\n",
    "            age_group,\n",
    "            COUNT(category) AS category_count,\n",
    "            RANK() OVER (PARTITION BY age_group ORDER BY COUNT(category) DESC) AS rank\n",
    "        FROM (\n",
    "               SELECT df_temp_pin.category AS category,\n",
    "                       CASE \n",
    "                         WHEN df_temp_user.age BETWEEN 18 AND 24 THEN '18-24'\n",
    "                         WHEN df_temp_user.age BETWEEN 25 AND 35 THEN '25-35'\n",
    "                         WHEN df_temp_user.age BETWEEN 36 AND 50 THEN '36-50'\n",
    "                         ELSE '50+'\n",
    "                       END AS age_group\n",
    "               FROM df_temp_user\n",
    "               JOIN df_temp_pin ON df_temp_pin.ind = df_temp_user.ind\n",
    "             )\n",
    "        GROUP BY age_group, category\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "result_df_4 = spark.sql(cte_query + \"\"\"\n",
    "    SELECT age_group,\n",
    "           category,\n",
    "           category_count\n",
    "    FROM ranking_table \n",
    "    WHERE rank == 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the median follower count for users in different age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_pin.createOrReplaceTempView('df_temp_pin')\n",
    "df_user.createOrReplaceTempView('df_temp_user')\n",
    "\n",
    "# SQL query \n",
    "\n",
    "result_df_5 = spark.sql(\"\"\"\n",
    "             SELECT\n",
    "                   CASE \n",
    "                        WHEN df_temp_user.age BETWEEN 18 AND 24 THEN '18-24'\n",
    "                        WHEN df_temp_user.age BETWEEN 25 AND 35 THEN '25-35'\n",
    "                        WHEN df_temp_user.age BETWEEN 36 AND 50 THEN '36-50'\n",
    "                        ELSE '50+'\n",
    "                   END AS age_group,\n",
    "                   PERCENTILE(df_temp_pin.follower_count, 0.5) AS median_follower_count\n",
    "             FROM df_temp_user\n",
    "             JOIN df_temp_pin ON df_temp_pin.ind = df_temp_user.ind\n",
    "             GROUP BY age_group\n",
    "             ORDER BY age_group ASC\n",
    "             \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many users have joined between 2015 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_user.createOrReplaceTempView('df_temp_user')\n",
    "\n",
    "# SQL query \n",
    "\n",
    "result_df_6 = spark.sql(\"\"\"\n",
    "                      SELECT year(date_joined) AS year,\n",
    "                             COUNT(ind) AS number_users_joined\n",
    "                      FROM df_temp_user\n",
    "                      WHERE year(date_joined) BETWEEN 2015 AND 2020\n",
    "                      GROUP BY year(date_joined)\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the median follower count of users have joined between 2015 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_pin.createOrReplaceTempView('df_temp_pin')\n",
    "df_user.createOrReplaceTempView('df_temp_user')\n",
    "\n",
    "# SQL query \n",
    "\n",
    "result_df_7 = spark.sql(\"\"\"\n",
    "                      SELECT year(df_temp_user.date_joined) AS year,\n",
    "                             PERCENTILE(df_temp_pin.follower_count, 0.5) AS median_follower_count\n",
    "                      FROM df_temp_user\n",
    "                      JOIN df_temp_pin ON df_temp_pin.ind = df_temp_user.ind\n",
    "                      WHERE year(date_joined) BETWEEN 2015 AND 2020\n",
    "                      GROUP BY year(df_temp_user.date_joined)\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating temporary dataframes\n",
    "df_pin.createOrReplaceTempView('df_temp_pin')\n",
    "df_user.createOrReplaceTempView('df_temp_user')\n",
    "\n",
    "# SQL query \n",
    "\n",
    "result_df_8 = spark.sql(\"\"\"\n",
    "                      SELECT \n",
    "                             CASE \n",
    "                                WHEN df_temp_user.age BETWEEN 18 AND 24 THEN '18-24'\n",
    "                                WHEN df_temp_user.age BETWEEN 25 AND 35 THEN '25-35'\n",
    "                                WHEN df_temp_user.age BETWEEN 36 AND 50 THEN '36-50'\n",
    "                                ELSE '50+'\n",
    "                             END AS age_group,\n",
    "                             year(df_temp_user.date_joined) AS year,\n",
    "                             PERCENTILE(df_temp_pin.follower_count, 0.5) AS median_follower_count\n",
    "                      FROM df_temp_user\n",
    "                      JOIN df_temp_pin ON df_temp_pin.ind = df_temp_user.ind\n",
    "                      WHERE year(df_temp_user.date_joined) BETWEEN 2015 AND 2020\n",
    "                      GROUP BY year, age_group\n",
    "                      ORDER BY age_group ASC, year ASC\n",
    "                      \"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
